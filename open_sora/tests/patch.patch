diff --git a/opensora/models/layers/blocks.py b/opensora/models/layers/blocks.py
index 40e6abb..74bb1ce 100644
--- a/opensora/models/layers/blocks.py
+++ b/opensora/models/layers/blocks.py
@@ -19,7 +19,6 @@ import torch.distributed as dist
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.utils.checkpoint
-import xformers.ops
 from einops import rearrange
 from timm.models.vision_transformer import Mlp
 
@@ -28,6 +27,18 @@ from opensora.acceleration.parallel_states import get_sequence_parallel_group
 
 approx_gelu = lambda: nn.GELU(approximate="tanh")
 
+def memory_efficient_attention(q, k, v, p=0.0, attn_bias=None):
+    scale = 1.0 / q.shape[-1] ** 0.5
+    q = q * scale
+    q = q.transpose(1, 2)
+    k = k.transpose(1, 2)
+    v = v.transpose(1, 2)
+    attn = q @ k.transpose(-2, -1)
+    if attn_bias is not None:
+        attn = attn + attn_bias
+    attn = attn.softmax(-1)
+    attn = attn @ v
+    return attn.transpose(1, 2).contiguous()
 
 class LlamaRMSNorm(nn.Module):
     def __init__(self, hidden_size, eps=1e-6):
@@ -47,15 +58,7 @@ class LlamaRMSNorm(nn.Module):
 
 
 def get_layernorm(hidden_size: torch.Tensor, eps: float, affine: bool, use_kernel: bool):
-    if use_kernel:
-        try:
-            from apex.normalization import FusedLayerNorm
-
-            return FusedLayerNorm(hidden_size, elementwise_affine=affine, eps=eps)
-        except ImportError:
-            raise RuntimeError("FusedLayerNorm not available. Please install apex.")
-    else:
-        return nn.LayerNorm(hidden_size, eps, elementwise_affine=affine)
+    return nn.LayerNorm(hidden_size, eps, elementwise_affine=affine)
 
 
 def modulate(norm_func, x, shift, scale):
@@ -163,7 +166,7 @@ class Attention(nn.Module):
         if rope is not None:
             self.rope = True
             self.rotary_emb = rope
-        
+
         self.is_causal = False
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
@@ -187,38 +190,21 @@ class Attention(nn.Module):
                 q = self.rotary_emb(q)
                 k = self.rotary_emb(k)
 
-        if enable_flash_attn:
-            from flash_attn import flash_attn_func
-
-            # (B, #heads, N, #dim) -> (B, N, #heads, #dim)
-            q = q.permute(0, 2, 1, 3)
-            k = k.permute(0, 2, 1, 3)
-            v = v.permute(0, 2, 1, 3)
-            x = flash_attn_func(
-                q,
-                k,
-                v,
-                dropout_p=self.attn_drop.p if self.training else 0.0,
-                softmax_scale=self.scale,
-                causal=self.is_causal,
-            )
-        else:
-            dtype = q.dtype
-            q = q * self.scale
-            attn = q @ k.transpose(-2, -1)  # translate attn to float32
-            attn = attn.to(torch.float32)
-            if self.is_causal:
-                causal_mask = torch.tril(torch.ones_like(attn), diagonal=0)
-                causal_mask = torch.where(causal_mask.bool(), 0, float('-inf'))
-                attn += causal_mask
-            attn = attn.softmax(dim=-1)
-            attn = attn.to(dtype)  # cast back attn to original dtype
-            attn = self.attn_drop(attn)
-            x = attn @ v
+        dtype = q.dtype
+        q = q * self.scale
+        attn = q @ k.transpose(-2, -1)  # translate attn to float32
+        attn = attn.to(torch.float32)
+        if self.is_causal:
+            causal_mask = torch.tril(torch.ones_like(attn), diagonal=0)
+            causal_mask = torch.where(causal_mask.bool(), 0, float('-inf'))
+            attn += causal_mask
+        attn = attn.softmax(dim=-1)
+        attn = attn.to(dtype)  # cast back attn to original dtype
+        attn = self.attn_drop(attn)
+        x = attn @ v
 
         x_output_shape = (B, N, C)
-        if not enable_flash_attn:
-            x = x.transpose(1, 2)
+        x = x.transpose(1, 2)
         x = x.reshape(x_output_shape)
         x = self.proj(x)
         x = self.proj_drop(x)
@@ -314,41 +300,22 @@ class KVCompressAttention(nn.Module):
 
         q, k = self.q_norm(q), self.k_norm(k)
 
-        if enable_flash_attn:
-            from flash_attn import flash_attn_func
-
-            x = flash_attn_func(
-                q,
-                k,
-                v,
-                dropout_p=self.attn_drop.p if self.training else 0.0,
-                softmax_scale=self.scale,
-            )
-
-        elif self.mem_eff_attention:
-            attn_bias = None
-            if mask is not None:
-                attn_bias = torch.zeros([B * self.num_heads, q.shape[1], k.shape[1]], dtype=q.dtype, device=q.device)
-                attn_bias.masked_fill_(mask.squeeze(1).repeat(self.num_heads, 1, 1) == 0, float("-inf"))
-            x = xformers.ops.memory_efficient_attention(q, k, v, p=self.attn_drop.p, attn_bias=attn_bias)
-        else:
-            # (B, N, #heads, #dim) -> (B, #heads, N, #dim)
-            q = q.permute(0, 2, 1, 3)
-            k = k.permute(0, 2, 1, 3)
-            v = v.permute(0, 2, 1, 3)
-            dtype = q.dtype
-            q = q * self.scale
-            attn = q @ k.transpose(-2, -1)  # translate attn to float32
-            if not self.attn_half:
-                attn = attn.to(torch.float32)
-            attn = attn.softmax(dim=-1)
-            attn = attn.to(dtype)  # cast back attn to original dtype
-            attn = self.attn_drop(attn)
-            x = attn @ v
+        # (B, N, #heads, #dim) -> (B, #heads, N, #dim)
+        q = q.permute(0, 2, 1, 3)
+        k = k.permute(0, 2, 1, 3)
+        v = v.permute(0, 2, 1, 3)
+        dtype = q.dtype
+        q = q * self.scale
+        attn = q @ k.transpose(-2, -1)  # translate attn to float32
+        if not self.attn_half:
+            attn = attn.to(torch.float32)
+        attn = attn.softmax(dim=-1)
+        attn = attn.to(dtype)  # cast back attn to original dtype
+        attn = self.attn_drop(attn)
+        x = attn @ v
 
         x_output_shape = (B, N, C)
-        if not enable_flash_attn:
-            x = x.transpose(1, 2)
+        x = x.transpose(1, 2)
         x = x.reshape(x_output_shape)
         x = self.proj(x)
         x = self.proj_drop(x)
@@ -392,49 +359,28 @@ class SeqParallelAttention(Attention):
         # [B, SUB_N, 3, NUM_HEAD, HEAD_DIM] -> [B, N, 3, NUM_HEAD_PER_DEVICE, HEAD_DIM]
         qkv = all_to_all(qkv, sp_group, scatter_dim=3, gather_dim=1)
 
-        if self.enable_flash_attn:
-            qkv_permute_shape = (
-                2,
-                0,
-                1,
-                3,
-                4,
-            )  # [3, B, N, NUM_HEAD_PER_DEVICE, HEAD_DIM]
-        else:
-            qkv_permute_shape = (
-                2,
-                0,
-                3,
-                1,
-                4,
-            )  # [3, B, NUM_HEAD_PER_DEVICE, N, HEAD_DIM]
+        qkv_permute_shape = (
+            2,
+            0,
+            3,
+            1,
+            4,
+        )  # [3, B, NUM_HEAD_PER_DEVICE, N, HEAD_DIM]
         qkv = qkv.permute(qkv_permute_shape)
 
         # ERROR: Should qk_norm first
         q, k, v = qkv.unbind(0)
         q, k = self.q_norm(q), self.k_norm(k)
-        if self.enable_flash_attn:
-            from flash_attn import flash_attn_func
-
-            x = flash_attn_func(
-                q,
-                k,
-                v,
-                dropout_p=self.attn_drop.p if self.training else 0.0,
-                softmax_scale=self.scale,
-            )
-        else:
-            dtype = q.dtype
-            q = q * self.scale
-            attn = q @ k.transpose(-2, -1)  # translate attn to float32
-            attn = attn.to(torch.float32)
-            attn = attn.softmax(dim=-1)
-            attn = attn.to(dtype)  # cast back attn to original dtype
-            attn = self.attn_drop(attn)
-            x = attn @ v
+        dtype = q.dtype
+        q = q * self.scale
+        attn = q @ k.transpose(-2, -1)  # translate attn to float32
+        attn = attn.to(torch.float32)
+        attn = attn.softmax(dim=-1)
+        attn = attn.to(dtype)  # cast back attn to original dtype
+        attn = self.attn_drop(attn)
+        x = attn @ v
 
-        if not self.enable_flash_attn:
-            x = x.transpose(1, 2)
+        x = x.transpose(1, 2)
 
         # apply all to all to gather back attention heads and split sequence
         # [B, N, NUM_HEAD_PER_DEVICE, HEAD_DIM]  -> [B, SUB_N, NUM_HEAD, HEAD_DIM]
@@ -467,14 +413,14 @@ class MultiHeadCrossAttention(nn.Module):
         # query/value: img tokens; key: condition; mask: if padding tokens
         B, N, C = x.shape
 
-        q = self.q_linear(x).view(1, -1, self.num_heads, self.head_dim)
-        kv = self.kv_linear(cond).view(1, -1, 2, self.num_heads, self.head_dim)
+        q = self.q_linear(x).view(B, -1, self.num_heads, self.head_dim)
+        kv = self.kv_linear(cond).view(B, -1, 2, self.num_heads, self.head_dim)
         k, v = kv.unbind(2)
 
         attn_bias = None
-        if mask is not None:
-            attn_bias = xformers.ops.fmha.BlockDiagonalMask.from_seqlens([N] * B, mask)
-        x = xformers.ops.memory_efficient_attention(q, k, v, p=self.attn_drop.p, attn_bias=attn_bias)
+        #if mask is not None:
+        #attn_bias = xformers.ops.fmha.BlockDiagonalMask.from_seqlens([N] * B, mask)
+        x = memory_efficient_attention(q, k, v, p=self.attn_drop.p, attn_bias=attn_bias)
 
         x = x.view(B, -1, C)
         x = self.proj(x)
@@ -522,7 +468,7 @@ class SeqParallelMultiHeadCrossAttention(MultiHeadCrossAttention):
         attn_bias = None
         if mask is not None:
             attn_bias = xformers.ops.fmha.BlockDiagonalMask.from_seqlens([N] * B, mask)
-        x = xformers.ops.memory_efficient_attention(q, k, v, p=self.attn_drop.p, attn_bias=attn_bias)
+        x = memory_efficient_attention(q, k, v, p=self.attn_drop.p, attn_bias=attn_bias)
 
         # apply all to all to gather back attention heads and scatter sequence
         x = x.view(B, -1, self.num_heads // sp_size, self.head_dim)
diff --git a/opensora/models/vae/__init__.py b/opensora/models/vae/__init__.py
index f27be47..d11ddf3 100644
--- a/opensora/models/vae/__init__.py
+++ b/opensora/models/vae/__init__.py
@@ -1,3 +1,4 @@
 from .discriminator import DISCRIMINATOR_3D
 from .vae import VideoAutoencoderKL, VideoAutoencoderKLTemporalDecoder
 from .vae_temporal import VAE_Temporal
+from .vae import OpenSoraVAE_V1_2
diff --git a/scripts/inference.py b/scripts/inference.py
index c4578a7..d43024e 100644
--- a/scripts/inference.py
+++ b/scripts/inference.py
@@ -47,11 +47,9 @@ def main():
     cfg_dtype = cfg.get("dtype", "fp32")
     assert cfg_dtype in ["fp16", "bf16", "fp32"], f"Unknown mixed precision {cfg_dtype}"
     dtype = to_torch_dtype(cfg.get("dtype", "bf16"))
-    torch.backends.cuda.matmul.allow_tf32 = True
-    torch.backends.cudnn.allow_tf32 = True
 
     # == init distributed env ==
-    if is_distributed():
+    if False:
         colossalai.launch_from_torch({})
         coordinator = DistCoordinator()
         enable_sequence_parallelism = coordinator.world_size > 1
